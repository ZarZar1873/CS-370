{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.5206 - accuracy: 0.8425 - val_loss: 0.1861 - val_accuracy: 0.9452\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.2297 - accuracy: 0.9317 - val_loss: 0.1399 - val_accuracy: 0.9588\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1791 - accuracy: 0.9464 - val_loss: 0.1128 - val_accuracy: 0.9663\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1521 - accuracy: 0.9544 - val_loss: 0.1067 - val_accuracy: 0.9687\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.1312 - accuracy: 0.9603 - val_loss: 0.0996 - val_accuracy: 0.9710\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.1188 - accuracy: 0.9637 - val_loss: 0.0901 - val_accuracy: 0.9741\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.1070 - accuracy: 0.9679 - val_loss: 0.0903 - val_accuracy: 0.9729\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.0988 - accuracy: 0.9692 - val_loss: 0.0854 - val_accuracy: 0.9738\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0897 - accuracy: 0.9718 - val_loss: 0.0875 - val_accuracy: 0.9746\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.0839 - accuracy: 0.9735 - val_loss: 0.0808 - val_accuracy: 0.9761\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.0801 - accuracy: 0.9739 - val_loss: 0.0851 - val_accuracy: 0.9767\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.0766 - accuracy: 0.9754 - val_loss: 0.0814 - val_accuracy: 0.9772\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.0720 - accuracy: 0.9763 - val_loss: 0.0815 - val_accuracy: 0.9774\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 73us/step - loss: 0.0699 - accuracy: 0.9780 - val_loss: 0.0906 - val_accuracy: 0.9762\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.0681 - accuracy: 0.9776 - val_loss: 0.0800 - val_accuracy: 0.9780\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.0649 - accuracy: 0.9789 - val_loss: 0.0802 - val_accuracy: 0.9772\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.0599 - accuracy: 0.9798 - val_loss: 0.0804 - val_accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.0581 - accuracy: 0.9814 - val_loss: 0.0787 - val_accuracy: 0.9782\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0554 - accuracy: 0.9822 - val_loss: 0.0871 - val_accuracy: 0.9769\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.0557 - accuracy: 0.9822 - val_loss: 0.0798 - val_accuracy: 0.9790\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Test score: 0.07218001167836992\n",
      "Test accuracy: 0.9807999730110168\n"
     ]
    }
   ],
   "source": [
    "# Author: Dominic Drury\n",
    "# Example from textbook, Deep Learning with Keras\n",
    "#SNHU CS-370\n",
    "\n",
    "# epoch of 20, 2 hidden layers\n",
    "\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import RMSprop, Adam \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # Adam optimizer, explained in chapter 1 of Deep Learning with Keras\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60,000 rows of 28x28 values --> reshaped is 60,000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# N_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy'])\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size = BATCH_SIZE, epochs = NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# evaluating the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.8211 - accuracy: 0.7485 - val_loss: 0.2704 - val_accuracy: 0.9209\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3300 - accuracy: 0.9023 - val_loss: 0.1918 - val_accuracy: 0.9436\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2514 - accuracy: 0.9257 - val_loss: 0.1544 - val_accuracy: 0.9548\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2067 - accuracy: 0.9385 - val_loss: 0.1370 - val_accuracy: 0.9604\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.1794 - accuracy: 0.9470 - val_loss: 0.1223 - val_accuracy: 0.9646\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1597 - accuracy: 0.9528 - val_loss: 0.1111 - val_accuracy: 0.9678\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1435 - accuracy: 0.9561 - val_loss: 0.1052 - val_accuracy: 0.9692\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1320 - accuracy: 0.9595 - val_loss: 0.1032 - val_accuracy: 0.9689\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.1230 - accuracy: 0.9620 - val_loss: 0.0960 - val_accuracy: 0.9715\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1151 - accuracy: 0.9647 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1062 - accuracy: 0.9672 - val_loss: 0.0897 - val_accuracy: 0.9734\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.1015 - accuracy: 0.9682 - val_loss: 0.0871 - val_accuracy: 0.9729\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0949 - accuracy: 0.9707 - val_loss: 0.0861 - val_accuracy: 0.9729\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.0866 - accuracy: 0.9728 - val_loss: 0.0858 - val_accuracy: 0.9741\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0836 - accuracy: 0.9737 - val_loss: 0.0822 - val_accuracy: 0.9761\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0811 - accuracy: 0.9741 - val_loss: 0.0814 - val_accuracy: 0.9754\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.97 - 3s 53us/step - loss: 0.0755 - accuracy: 0.9764 - val_loss: 0.0846 - val_accuracy: 0.9758\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0719 - accuracy: 0.9773 - val_loss: 0.0815 - val_accuracy: 0.9747\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0734 - accuracy: 0.9771 - val_loss: 0.0818 - val_accuracy: 0.9754\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0683 - accuracy: 0.9791 - val_loss: 0.0797 - val_accuracy: 0.9761\n",
      "10000/10000 [==============================] - 0s 39us/step\n",
      "Test score: 0.07348913295457023\n",
      "Test accuracy: 0.9768000245094299\n"
     ]
    }
   ],
   "source": [
    "# Changes made\n",
    "# Increased batch size to 512 (4x the size) and saw a decrease in accuracy because the larger size results in a slower but\n",
    "# more accurate error gradient with a slightly faster run time\n",
    "# \n",
    "# Information I found on the effect of batch size on accuracy\n",
    "# https://stackoverflow.com/questions/55485837/why-does-different-batch-sizes-give-different-accuracy-in-keras\n",
    "\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import RMSprop, Adam \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 512 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # Adam optimizer, explained in chapter 1 of Deep Learning with Keras\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60,000 rows of 28x28 values --> reshaped is 60,000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# N_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy'])\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size = BATCH_SIZE, epochs = NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# evaluating the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 16s 330us/step - loss: 0.3868 - accuracy: 0.8829 - val_loss: 0.1541 - val_accuracy: 0.9528\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 14s 287us/step - loss: 0.1931 - accuracy: 0.9416 - val_loss: 0.1190 - val_accuracy: 0.9648\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 13s 262us/step - loss: 0.1512 - accuracy: 0.9543 - val_loss: 0.1089 - val_accuracy: 0.9678\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 12s 259us/step - loss: 0.1319 - accuracy: 0.9605 - val_loss: 0.0903 - val_accuracy: 0.9722\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 13s 273us/step - loss: 0.1148 - accuracy: 0.9650 - val_loss: 0.0890 - val_accuracy: 0.9754\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 13s 266us/step - loss: 0.1063 - accuracy: 0.9676 - val_loss: 0.0877 - val_accuracy: 0.9732\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 12s 251us/step - loss: 0.0979 - accuracy: 0.9688 - val_loss: 0.0927 - val_accuracy: 0.9725\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 11s 224us/step - loss: 0.0902 - accuracy: 0.9722 - val_loss: 0.0840 - val_accuracy: 0.9758\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 14s 292us/step - loss: 0.0864 - accuracy: 0.9735 - val_loss: 0.0805 - val_accuracy: 0.9762\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 13s 278us/step - loss: 0.0812 - accuracy: 0.9737 - val_loss: 0.0770 - val_accuracy: 0.9764\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 13s 278us/step - loss: 0.0804 - accuracy: 0.9752 - val_loss: 0.0758 - val_accuracy: 0.9784\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 15s 312us/step - loss: 0.0739 - accuracy: 0.9770 - val_loss: 0.0804 - val_accuracy: 0.9778\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 13s 270us/step - loss: 0.0707 - accuracy: 0.9779 - val_loss: 0.0818 - val_accuracy: 0.9789\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 14s 285us/step - loss: 0.0681 - accuracy: 0.9785 - val_loss: 0.0861 - val_accuracy: 0.9765\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 13s 263us/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 0.0836 - val_accuracy: 0.9780\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 14s 286us/step - loss: 0.0648 - accuracy: 0.9793 - val_loss: 0.0804 - val_accuracy: 0.9796\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 15s 312us/step - loss: 0.0637 - accuracy: 0.9804 - val_loss: 0.0769 - val_accuracy: 0.9789\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 12s 256us/step - loss: 0.0584 - accuracy: 0.9814 - val_loss: 0.0910 - val_accuracy: 0.9781\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 11s 234us/step - loss: 0.0580 - accuracy: 0.9810 - val_loss: 0.0919 - val_accuracy: 0.9774\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 11s 236us/step - loss: 0.0552 - accuracy: 0.9825 - val_loss: 0.0887 - val_accuracy: 0.9784\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "Test score: 0.08413783750284946\n",
      "Test accuracy: 0.9783999919891357\n"
     ]
    }
   ],
   "source": [
    "# Changes made\n",
    "# Decreased batch size to 32 (1/4 the size) and saw a decrease in accuracy because the smaller size results in a quicker but\n",
    "# less accurate error gradient but the compile time was significantly higher\n",
    "# \n",
    "# Information I found on the effect of batch size on accuracy\n",
    "# https://stackoverflow.com/questions/55485837/why-does-different-batch-sizes-give-different-accuracy-in-keras\n",
    "\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import RMSprop, Adam \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "\n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 32 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # Adam optimizer, explained in chapter 1 of Deep Learning with Keras\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60,000 rows of 28x28 values --> reshaped is 60,000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "# \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# N_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "optimizer=OPTIMIZER, \n",
    "metrics=['accuracy'])\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size = BATCH_SIZE, epochs = NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# evaluating the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
